Готовое решение для управления логами в Kubernetes. Позволяет эффективно хранить большие объёмы записей (в СУБД [ClickHouse](https://github.com/yandex/ClickHouse)) и обрабатывать их с помощью простого языка запросов, а также просматривать логи в реальном времени (веб-интерфейс). Быстро и легко разворачивается в работающем кластере K8s.

Официальный статус — **альфа-версия**, но мы (Флант) используем в production с сентября 2017 года.

Обновления альфа-версии могут изменять/повреждать данные. Смотрите release notes при обновлении на новую версию.
**Обновления будут гарантированно стабильными начиная с бета-версии** (запланирована на апрель 2018)

Демонстрация работы интерфейса loghouse-dashboard в действии (~3 Мб):

![loghouse web UI](https://cdn.rawgit.com/flant/loghouse/master/docs/web-ui-animated.gif)

# Возможности

* Эффективный сбор и хранение логов в Kubernetes:
  * Используемый в решении [fluentd](https://www.fluentd.org/) потребляет 300 Мб памяти (на каждом узле), обрабатывая при этом до 10 тысяч записей в секунду.
  * Примеры по объему места, занимаемому логами в ClickHouse (из реальных инсталляций): 3,7 млн записей — 1,2 Гб, 300 млн — 13 Гб, 5,35 млрд — 54 Гб.
* Простой язык запросов. Позволяет отбирать записи с сопоставлением ключей с конкретными значениями и регулярными выражениями, поддерживает множество условий через AND/OR. *Подробнее см. в [документации](query-language.md).*
* Возможность выбора записей не только по содержимому/времени логов, но и по данным о контейнерах из Kubernetes API (имя пода и контейнера, хост, пространство имён, лейблы и т.п.).
* Простой деплой в Kubernetes с помощью готовых Dockerfile и Helm-чарта.
* Удобный и мощный веб-интерфейс:
  * Внешний вид, выполненный в стиле Papertrail.
  * Выбор периода: за указанные даты/время или от текущего момента (последний час, день и т.п.), а также возможность указать время инцидента и посмотреть логи, которые были в это время.
  * Бесконечная прокрутка записей при просмотре предыдущих логов.
  * Сохранение произвольных запросов для их дальнейшего повторного использования.
  * Разделение доступа (ограничение пользователей указанными пространствами имен Kubernetes).
  * Экспорт результатов текущего запроса в CSV (в планах и другие форматы).

# Установка

Для установки loghouse потребуется наличие [Helm](https://github.com/kubernetes/helm) и выполнение двух шагов:

1. Добавление чартов для loghouse:
```
# helm repo add loghouse https://flant.github.io/loghouse/charts/
```

2. Установка чарта.

2.1. Простой путь:

```
# helm fetch loghouse/loghouse --untar
# vim loghouse/values.yaml
# helm install -n loghouse loghouse
```

2.2. С использованием специальных параметров *(см. переменные в [values.yaml](https://github.com/flant/loghouse/blob/master/charts/loghouse/values.yaml) чарта — документация будет готова позже)*:

```
# helm install -n loghouse loghouse/loghouse --set 'param=value' ...
```

После установки веб-интерфейс (loghouse-dashboard) будет доступен по домену, указанному в конфигурационном файле values.yaml как ```loghouse_host```. Потребуется пройти базовую аутентификацию, сгенерированную с помощью htpasswd и определенную в параметре ```auth``` конфигурационного файла values.yaml.

# Архитектура

![loghouse architecture](https://cdn.rawgit.com/flant/loghouse/master/docs/architecture.png)

На каждый узел кластера Kubernetes устанавливается под с fluentd для сбора логов. Технически для этого в Kubernetes создается DaemonSet, который имеет tolerations для всех возможных taints и попадает на все узлы кластера. Каталоги с логами со всех хост-систем монтируются в поды fluentd из этого DaemonSet, где за ними «наблюдает» служба fluentd. Для всех логов Docker-контейнеров применяется фильтр [kubernetes_metadata](https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter), который собирает дополнительную информацию о контейнерах из Kubernetes API. После этого данные преобразуются с помощью фильтра [record_modifier](https://github.com/repeatedly/fluent-plugin-record-modifier). После преобразования данных они попадают в fluentd output plugin, который вызывает расположенную в контейнере с fluentd консольную утилиту [clickhouse-client](https://clickhouse.yandex/docs/en/interfaces/cli.html) для записи данных в ClickHouse.

**Примечание про формат логов**: Если лог имеет формат JSON, то он форматируется по типу значений, т.е. каждое поле попадает в одну из таблиц: string_fields, number_fields, boolean_fields, null_fields и labels (последняя — это лейблы контейнеров для удобной фильтрации и поиска) для возможности использования встроенных в ClickHouse функций для работы с этими типами данных. В случае, если лог не в формате JSON, он просто попадает в таблицу string_fields.

На данный момент поддерживается запись в единственный экземпляр СУБД ClickHouse — Deployment, который по умолчанию попадает на случайный узел K8s (можно задать nodeSelector и tolerations для выбора конкретного узла). ClickHouse хранит свои данные в hostPath или в Persistent Volumes Claim (PVC), созданном с помощью выбранного storageClass.

Веб-интерфейс loghouse ([скриншот](http://screenshot.flant.ru/asidorovj/04/81/0481900cca291c45718bd6dbae66c64b98fa0b32.png)) состоит из двух компонентов:

* **frontend** — nginx с базовой авторизацией. На основе данной авторизации можно выдавать права доступа на отображение логов для определенного пользователя, ограниченные конкретными пространствами имен Kubernetes;
* **backend** — приложение на Ruby, которое выполняет всю работу по выводу логов из ClickHouse.

# План развития

Мы собираемся добавить другие варианты использования ClickHouse (экземляры СУБД на каждом узле K8s или кластер ClickHouse), перевести frontend на AngularJS, а backend — на Golang, добавить консольный интерфейс (CLI) и многое другое.

Более подробный план появится в ближайшее время в виде [issues](https://github.com/flant/loghouse/issues) проекта.

# Дополнительная документация

* [Язык запросов](query-language.md)
* [FAQ](FAQ.md)
