Готовое решение для управления логами в Kubernetes. Позволяет эффективно хранить большие объёмы записей (в СУБД [ClickHouse](https://github.com/yandex/ClickHouse)) и обрабатывать их с помощью простого языка запросов, а также просматривать логи в реальном времени (веб-интерфейс). Быстро и легко разворачивается в работающем кластере K8s.

Официальный статус — **альфа-версия**, но мы (Флант) используем в production с сентября 2017 года.

Обновления альфа-версии могут изменять/повреждать данные. Смотрите release notes при обновлении на новую версию. **Обновления будут гарантированно стабильными начиная с бета-версии.**

Демонстрация работы интерфейса loghouse-dashboard в действии (~3 Мб):

![loghouse web UI](https://cdn.rawgit.com/flant/loghouse/master/docs/web-ui-animated.gif)

# Возможности

* Эффективный сбор и хранение логов в Kubernetes:
  * Используемый в решении [fluentd](https://www.fluentd.org/) потребляет 300 Мб памяти (на каждом узле), обрабатывая при этом до 10 тысяч записей в секунду.
  * Примеры по объему места, занимаемому логами в ClickHouse (из реальных инсталляций): 3,7 млн записей — 1,2 Гб, 300 млн — 13 Гб, 5,35 млрд — 54 Гб.
* Простой язык запросов. Позволяет отбирать записи с сопоставлением ключей с конкретными значениями и регулярными выражениями, поддерживает множество условий через AND/OR. *Подробнее см. в [документации](query-language.md).*
* Возможность выбора записей не только по содержимому/времени логов, но и по данным о контейнерах из Kubernetes API (имя пода и контейнера, хост, пространство имён, лейблы и т.п.).
* Простой деплой в Kubernetes с помощью готовых Dockerfile и Helm-чарта.
* Удобный и мощный веб-интерфейс:
  * Внешний вид, выполненный в стиле Papertrail.
  * Выбор периода: за указанные даты/время или от текущего момента (последний час, день и т.п.), а также возможность указать время инцидента и посмотреть логи, которые были в это время.
  * Бесконечная прокрутка записей при просмотре предыдущих логов.
  * Сохранение произвольных запросов для их дальнейшего повторного использования.
  * Разделение доступа (ограничение пользователей указанными пространствами имен Kubernetes).
  * Экспорт результатов текущего запроса в CSV (в планах и другие форматы).

# Установка

loghouse устанавливается с помощью [Helm](https://github.com/kubernetes/helm). Минимальная поддерживаемая версия kubernetes **>=1.9**. Так же в кластере уже должен быть установлен [cert-manager](https://github.com/jetstack/cert-manager), чтобы выпускать сертификаты.

Для установки выполните два шага:

1. Добавление чартов для loghouse:
```
# helm repo add loghouse https://flant.github.io/loghouse/charts/
```

2. Установка чарта.

2.1. Простой путь:

```
# helm fetch loghouse/loghouse --untar
# vim loghouse/values.yaml
# helm install -n loghouse loghouse
```

2.2. С использованием специальных параметров *(см. переменные в [values.yaml](https://github.com/flant/loghouse/blob/master/charts/loghouse/values.yaml) чарта — документация будет готова позже)*:

```
# helm install -n loghouse loghouse/loghouse --set 'param=value' ...
```

После установки веб-интерфейс (loghouse-dashboard) будет доступен по домену, указанному в конфигурационном файле values.yaml как ```loghouse_host```. Потребуется пройти базовую аутентификацию, сгенерированную с помощью htpasswd и определенную в параметре ```auth``` конфигурационного файла values.yaml.

# Обновление

При переходе на версию **0.3** Helm chart претерпел некоторые изменения. Мы изменили работу с хуками, а так же обновили версии API у объектов Kubernetes. Чтобы обновиться с предыдущей версии надо удалить активные job, а так же ingress. Сделать это можно вот такой командой
```
kubectl -n loghouse delete jobs,ing --all
```

**Важно знать!** Схема базы данных изменилась, поэтому обязательно сделайте бэкап данных в clickhouse перед обновлением. В процессе обновления будет запущена миграция, которая изменит структуру базы и некоторое время будет переносить данные в новые таблицы. 

# Архитектура

![loghouse architecture](docs/architecture.png)

На каждый узел кластера Kubernetes устанавливается под с fluentd для сбора логов. Технически для этого в Kubernetes создается DaemonSet, который имеет tolerations для всех возможных taints и попадает на все узлы кластера. Каталоги с логами со всех хост-систем монтируются в поды fluentd из этого DaemonSet, где за ними «наблюдает» служба fluentd. Для всех логов Docker-контейнеров применяется фильтр [kubernetes_metadata](https://github.com/fabric8io/fluent-plugin-kubernetes_metadata_filter), который собирает дополнительную информацию о контейнерах из Kubernetes API. После этого данные преобразуются с помощью фильтра [record_modifier](https://github.com/repeatedly/fluent-plugin-record-modifier). После преобразования данных они попадают в fluentd output plugin, который вызывает расположенную в контейнере с fluentd консольную утилиту [clickhouse-client](https://clickhouse.yandex/docs/en/interfaces/cli.html) для записи данных в ClickHouse.

**Примечание про формат логов**: Если лог имеет формат JSON, то он форматируется по типу значений, т.е. каждое поле попадает в одну из таблиц: string_fields, number_fields, boolean_fields, null_fields и labels (последняя — это лейблы контейнеров для удобной фильтрации и поиска) для возможности использования встроенных в ClickHouse функций для работы с этими типами данных. В случае, если лог не в формате JSON, он просто попадает в таблицу string_fields.

На данный момент поддерживается запись в единственный экземпляр СУБД ClickHouse — StatefulSet, который по умолчанию попадает на случайный узел K8s (можно задать nodeSelector и tolerations для выбора конкретного узла). ClickHouse хранит свои данные в hostPath или в Persistent Volumes Claim (PVC), созданном с помощью выбранного storageClass. Вы можете попробовать альтернативные для clickhouse. Примеры схем есть в [документации](docs/en/schemas/README.md)

Веб-интерфейс loghouse ([скриншот](docs/loghouse_interface.png)) состоит из двух компонентов:

* **frontend** — nginx с базовой авторизацией. На основе данной авторизации можно выдавать права доступа на отображение логов для определенного пользователя, ограниченные конкретными пространствами имен Kubernetes;
* **backend** — приложение на Ruby, которое выполняет всю работу по выводу логов из ClickHouse.

Clickhouse и Fluentd можно подключить к Prometheus. Примены панелей для Grafana вы можете найти [тут](docs/en/grafana)

# План развития

Мы собираемся добавить другие варианты использования ClickHouse (экземляры СУБД на каждом узле K8s или кластер ClickHouse), перевести frontend на AngularJS, а backend — на Golang, добавить консольный интерфейс (CLI) и многое другое.

Более подробный план появится в ближайшее время в виде [issues](https://github.com/flant/loghouse/issues) проекта.

# Дополнительная документация

* [Язык запросов](query-language.md)
* [FAQ](FAQ.md)
